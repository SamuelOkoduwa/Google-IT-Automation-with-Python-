# Deploying containers to GCP

A Kubernetes cluster is a fundamental construct within Kubernetes. The cluster enables the deployment, coordination, and operation of containerised applications at scale. Instead of one incredibly huge, ridiculously fast server positioned in one place to process requests from all around the world, clusters are lots of smaller servers spread out and coordinated to serve everyone close to where they are. 

A cluster is a group of machines grouped to work together, but not necessarily all doing the same tasks. In a Kubernetes cluster, virtual machines (VMs) are coordinated to execute all of the functions needed to process requests, such as serving a web application, running a database, or solving big-data problems. Each cluster consists of at least one cluster control plane machine, a server that manages multiple nodes. You submit all of your work to the control plane, and the control plane distributes the work to the node or nodes where it will run. These worker nodes are virtual machine (VM) instances running the Kubernetes processes necessary to make them part of the cluster. They can be in a single zone or spread out all over the world. Depending on the use case, one node might be used for data processing and another for hosting a web server. Each of these nodes is made up of pods, which are assigned by the control plane. Each pod is made up of one or more containers that work together to execute necessary functions. 

Have you ever been to a restaurant where there is a rolling tray of desserts? Well, the tray is like a cluster, holding all the little plates. The plates are like nodes, each working to hold a different dessert. The desserts are like the pods, each performing a different flavor, or function. 

It’s easy to create a Kubernetes cluster on GCP that meets your exact requirements. Kubernetes clusters allow multiple nodes to work together in concert no matter their physical distance. 

---
##Kubernetes clusters
A Kubernetes cluster comprises multiple servers (which Kubernetes calls “nodes”) that work together as a group. These nodes are virtual or physical machines that form the underlying infrastructure of the Kubernetes cluster. 

Each node is capable of running containers and hosting workloads. Kubernetes clusters are designed for scalability and high availability. Nodes can be added or removed as needed as workloads vary, so applications can scale up or down seamlessly. 

Nodes are interconnected and communicate with each other through the Kubernetes control plane to ensure seamless coordination and collaboration. The control plane is the brain of the Kubernetes cluster. It consists of several components that manage and monitor the cluster's overall state, including:

- An API server

- A controller manager

- A scheduler

- An etcd: This is a reliable data storage that can be accessed by the cluster of machines. 

Every Kubernetes cluster has one control plane and at least one control plane node. However, multiple nodes can be tasked with running the control plane components, and these component nodes can be spread out across zones for redundancy. 

The standard unit for deployment to a Kubernetes cluster is a container. Containerized applications are software applications packaged along with their dependencies, libraries, and configurations into isolated containers. Containers can be easily duplicated, ensuring easy, consistent deployment across different environments. 

Kubernetes is a powerful orchestration platform designed to manage and scale containerised applications. Kubernetes automates the deployment, scaling, and management of containerised applications across the cluster's nodes. Kubernetes also manages resources across the cluster by optimally allocating CPU, memory, and storage based on application requirements. This ensures that resources are used efficiently, and it minimizes conflicts between applications. And Kubernetes also maintains the health of the cluster by employing features that automatically replace failed or unhealthy containers. 

To manage a Kubernetes cluster, users specify the desired state of their applications, and then the cluster handles the actual execution and maintenance of the applications to match that desired state. This is called a “declarative approach.” The declarative approach simplifies management and reduces the need for manual intervention once the initial parameters are set. 

#### Different types of Kubernetes clusters 
Selecting the right type of cluster ensures a well-aligned Kubernetes deployment that will meet your specific business needs and objectives. Here are some of the major cluster architectures:

-	**On-premises cluster**
An on-premises Kubernetes cluster is deployed within an organization's own data center or on a private infrastructure. Deploying an on-premises cluster involves setting up the control plane and worker nodes on the organization's own hardware, and the organization is responsible for cluster maintenance. This provides complete control over the hardware, networking, and security. This is particularly suitable for situations with specific compliance or data governance requirements. 
On-premises clusters are one of the primary types of Kubernetes clusters.  Tools like Kubernetes kubeadm and Kubernetes Operations (kOps) are designed for deploying this type of cluster, and there are multiple custom configurations that can be used to create and manage on-premises clusters

-	**Public cloud managed cluster**
Another primary type of Kubernetes cluster is a public cloud managed cluster. Public cloud providers offer managed Kubernetes services, handling the underlying infrastructure management so it is easier for users to deploy and manage Kubernetes clusters on the cloud. This is a useful option for teams that prefer to offload cluster management tasks, but who still require the scalability and flexibility of cloud-based deployments.  This type of cluster allows the organization to focus on deploying and managing applications without dealing with the complexities of cluster maintenance in the way you would with an on-premise cluster. When you run Kubernetes in the cloud, cluster maintenance is automatically handled by the cloud provider. You don't need to worry about it. 
Another advantage of public cloud managed clusters is that they can be spread over zones or even regions. Just by checking a box in your configurations, you can spread your clusters geographically in case a cloud data center goes down or there are network problems. Some examples of managed services by public cloud providers are Amazon Elastic Kubernetes Service (EKS) on AWS, Google Kubernetes Engine (GKE) on Google Cloud, and Azure Kubernetes Service (AKS) on Microsoft Azure. 

-	**Private cloud managed cluster**
These clusters function similarly to public cloud managed clusters, but private cloud providers manage Kubernetes services for deploying clusters within a private cloud environment. This combines the ease and flexibility of managed services with control over the private infrastructure. 
Some examples of managed private cloud providers are Nutanix, OpenStack, and Hewlett Packard Enterprises (HPE). 

-	**Local development clusters**
The third primary type of Kubernetes clusters are local development clusters. These are lightweight and easy-to-set-up Kubernetes environments which facilitate a fast development workflow for individual developers. These are most often set up as local development and testing clusters, and they’re primarily used for application development and testing on a developer's local machine. They are often employed during the development phase to enable rapid iteration and debugging of applications before deploying them to production clusters. 
Tools commonly used to create local development clusters include Minikube, Docker Desktop (with Kubernetes enabled), and Kubernetes kind (Kubernetes in Docker). Each of these tools allows developers to spin up a single-node Kubernetes cluster locally to develop and validate applications without the need for a full-scale production cluster.

-	**Hybrid cluster**
A hybrid Kubernetes cluster coordinates on-premises and cloud environments, allowing workloads to run seamlessly across both locations. This type of cluster is suitable for scenarios in which some applications need to reside on-premises, for instance for security requirements, while others benefit from cloud scalability and services. 

-	**Edge cluster**
An edge Kubernetes cluster is deployed at the edge of the network, closer to the locations of end-users or Internet of Things (IoT) devices. Edge clusters are designed to support low latency, particularly in regions or zones where power and network connectivity are scarce and expensive.

-	**High-performance computing (HPC) cluster**
HPC Kubernetes clusters are tailored for running computationally intensive workloads, such as scientific simulations or large data processing tasks. These clusters optimize performance by leveraging specialized hardware and configurations. 

-	**Multi-cluster federation**
Multi-cluster federation involves managing multiple Kubernetes clusters as a single logical cluster. This allows centralized management of workloads, which are deployed across clusters similarly to the way a single Kubernetes cluster distributes workloads to multiple nodes. Multi-cluster federation facilitates global-scale deployments like disaster recovery scenarios. 


In Summary, Kubernetes clusters are nodes which are coordinated to act as singular, cohesive units to provide a flexible and reliable platform. Clusters offer high availability, efficient scaling, and robust security. Depending on the computational purpose and requirements, you can configure a cluster to suit any organization.

---

####	Deploying Docker containers on GCP
Docker containers make it simple to deploy applications across different systems and platforms, allowing them to run the same way no matter what environment they are deployed to. This makes it easy to share, test, manage, and deploy applications quickly and reliably. 

There are several platforms that allow you to deploy Docker containers, and each has its own set of advantages. Let’s look at some of these, and some considerations when choosing where to deploy containers.

GCP offers several choices for deploying Docker containers, all of which allow you to integrate with other Google services. Cloud Run is the simplest to use, offering a fully managed platform, but with little customization. GKE is a powerful platform that offers more flexibility in configuration coupled with plenty of options for automation. Google Compute Engine lets you control your environments and applications while they run on Google’s infrastructure, but requires significantly more technical knowledge than Cloud Run or GKE. The best option for you will be based on your needs.  

---

#### Kubernetes YAML
Kubernetes YAML files play a crucial role in defining and managing Kubernetes resources, enabling Python developers to manage their applications' infrastructure in a consistent, version-controlled, and automated manner. By understanding the structure of these files and their key components, developers can leverage Kubernetes to its full potential and focus more on writing the application logic rather than managing infrastructure.

---

#### Scaling Containers on GCP
One of the most important factors in cloud computing is scalability. For a moment, let’s imagine cloud computing as if it were an actual cloud in the sky. Clouds are made up of water vapor; the more vapor that joins the cloud, the larger it gets. As the cloud releases this water as snow or rain, the cloud gets smaller. That’s scalability; the process of expanding or shrinking as necessary.

The conditions in the atmosphere determine the shape of a cloud throughout this scaling process. There is a lot more we could say about sky-clouds, but let’s return to computing clouds. Your application's requirements are the conditions that determine the direction and method of scaling in cloud computing. 

**Horizontal and vertical scaling**
Generally, we can talk about scaling on two axes, horizontal, meaning sideways, and vertical, meaning up and down. In horizontal scaling, more containers are added as needed. Horizontal scaling is useful for adding dedicated resources as the number of users increase, and creating fallback containers in case of failure. In vertical scaling, more resources are allocated to each container. Vertical scaling increases performance. 

**Multidimensional scaling**
Multidimensional scaling is a combination of horizontal and vertical scaling. This is sometimes called diagonal scaling, because you are doing some horizontal scaling, adding more containers to add to the number of resources, and some vertical scaling, increasing the performance of existing or added resources. 

Imagine it’s autumn and someone is burning a pile of leaves, but the fire is a little bigger than they planned. That’s pretty dangerous, so they call the fire department. Meanwhile, they fill a bucket and use it to throw water on the fire. Neighbors come over, each with their own bucket, and start throwing more buckets of water. That’s horizontal scaling, the addition of more small resources. 

The fire truck rolls up and hooks up the big hose, but the pressure isn’t very good. In fact it isn’t any better than the pressure from the hoses that other neighbors have stretched from their own homes. All these hoses are moving more water than the buckets, though. That’s diagonal scaling; more resources with a bit more performance in each. 

But then the fire department cranks open the valve on the fire hydrant, and a huge jet of water flies out of the big hose. That’s vertical scaling; increasing resources to a single response unit to increase its performance. 

**Elastic scaling**
In cloud computing, you can employ elastic scaling to automatically increase or decrease the number of servers (horizontal) or the resources allocated to existing servers (vertical) or both (multidimensional) based on the current demand. 

Containers can scale down to a fraction of a computer, or scale up to use all the resources of multiple computers. It’s important to decide on the type or types of scaling your application will require in advance so you can make sure to have the right service-level agreement (SLA). Your SLA is the contract with your platform provider; this dictates what will be furnished to you, and at what cost.

---
####	Container Security
Containers pose some unique security challenges, including securing the container runtime, protecting the host system, and managing application dependencies.

Adopting a Zero Trust model can help mitigate these challenges. This approach involves assuming no trust by default and only granting permissions as necessary, reducing the potential attack surface.

Security on GCP is a shared responsibility. GCP is responsible for infrastructure security, operational security, and providing tools for software supply chain security. Developers are responsible for workload security, network security, identity and access management, and effective use of software supply chain security tools.

GCP provides several security features and best practices for protecting containers, including using minimal base images, regularly updating and patching containers, implementing vulnerability scanning, using runtime security tools like gVisor, implementing access controls with IAM, encrypting sensitive data with KMS, monitoring and logging activity with Cloud Audit Logs, and using Binary Authorization to ensure only trusted images are deployed.

***