***
<h1>Kubernetes Principles</h1>

# Kubernetes P

***
Kubernetes is an open-sourced container orchestration platform that automates the deployment, scaling, and management of containerized applications. Kubernetes provides developers with a framework to easily run distributed systems. Kubernetes also provides developers choice and flexibility when building platforms.

Kubernetes—a cloud-native application—follows principles to ensure the containerized application runs properly. These principles take into consideration build time and run time. A container is self-contained, relying only on the Linux kernel. Once the container is built, then additional libraries can be added. In addition, containerized applications are not meant to change to different environments after they are built.

In terms of run time, each container needs to implement APIs to help the platform manage the application in the most efficient way possible. All APIs must be public, as there should be no hidden or private APIs. In addition, APIs should be declarative, meaning the programmer should be able to communicate their desired end result, allowing Kubernetes to find and implement a solution. Support is available to developers, if needed, to run applications in Kubernetes. Workloads are portable, and the control plane is able to transfer a workload to another node without disrupting the overall program.

**Declarative configuration**
Declarative configuration is an approach that is commonly used in Kubernetes to achieve a desired state of an application. In this approach, developers specify the desired state, but they do not explicitly define how to achieve or reach the desired state. The approach is more focused on what the desired state should be. The system will determine the most efficient and reliable way to achieve the desired state. These configuration assets are stored in a revision control system and track changes over time.

To use declarative configuration in Kubernetes, create a manifest that describes the desired state of an application. Then, the control plane will determine how to direct nodes in the cluster to achieve the desired state.

**The control plane**
The Kubernetes control plane is responsible for making decisions about the entire cluster and desired state and for ensuring the cluster’s components work together. Components of the control plane include: 
- etcd
- API server
- Scheduler
- Controller manager
- Cloud controller manager

etcd is used as Kubernetes backing store for all cluster data as a distributed database. This key-value store is highly available and designed to run on multiple nodes.

The Kubernetes API server acts as the front-end for developers and other components interacting with the cluster. It is responsible for ensuring requests to the API are properly authenticated and authorized.

The scheduler is a component of the control plane where pods are assigned to run on particular nodes in the cluster.

The control manager hosts multiple Kubernetes controllers. Each controller continuously monitors the current state of the cluster and works towards achieving the desired state.

The cloud controller manager is a control plane component that embeds cloud-specific control logic. It acts as the interface between Kubernetes and a specific cloud provider, managing the cloud’s resources.

In summary, Kubernetes is a portable and extensible platform to assist developers with containerized applications. Kubernetes core principles and key components support developers with starting, stopping, storing, building, and managing containers.

***

Installing Kubernetes
---
***

There are multiple ways to set up and run a Kubernetes cluster. Because Kubernetes acts as a set of containers that manages other containers, Kubenetes is not something you download. You decide on the installation type you need based on your programming requirements. 

**Enable Kubernetes**
After Docker is installed on your machine, follow the instructions below to run Kubernetes in Docker Desktop.
1. From the Docker Dashboard, select Settings.

2. Select Kubernetes from the left sidebar.

3. Select the checkbox next to Enable Kubernetes.

4. Select Apply & Restart to save the settings.

5. Select Install to complete the installation process.

The Kubernetes server runs as containers and installs the /usr/local/bin/kubect1 command on your machine.

And that’s it! Setting up Kubernetes on Docker Desktop is typically the most common way that developers use Kubernetes since Docker Desktop has built-in support for it. 

Kubernetes is not a replacement for Docker, but rather a tool that developers use while working in Docker. It can run and manage Docker containers, allowing developers to deploy, scale, and manage containerized applications across clusters.

***

Pods
---

***

In Kubernetes, a container is a lightweight, standalone, executable package that includes everything needed to run a piece of software, including the code, a runtime, libraries, environment variables, and system tools. Containers are isolated from each other and bundle their own software, libraries, and configuration files, but they share the operating system kernel with other containers. They are designed to be easily portable across different environments, which makes them ideal for consistent deployment across different platforms.

In the context of Kubernetes, containers are the smallest units of deployment that are scheduled and managed. They are encapsulated within Pods, which are the fundamental deployment units in a Kubernetes cluster. A Pod can contain one or more containers that need to run together on the same host and share the same network and storage resources, allowing them to communicate with each other using localhost.

Pods serve as an abstraction layer, allowing Kubernetes to schedule and orchestrate containers effectively. When a deployment requires multiple containers to work together on the same node, a Pod is created to ensure they are co-located and can communicate efficiently. This simplifies the deployment and management of containerised applications, making it easier to scale, monitor, and update as needed.

Also note that Pods in Kubernetes are considered to be ephemeral; they can be created, terminated, and replaced dynamically based on the desired state and resource availability in the cluster. As a result, Kubernetes ensures that the desired number of Pods are always running, enabling high availability and fault tolerance for containerised applications.

Pods serve together as a logical host that encapsulates one or more tightly coupled containers within a shared network and storage context. This provides a way to group containers that need to work closely together, allowing them to share the same resources and interact with each other as if they were running on the same physical or virtual machine.

When designing a Pod, consider aspects like resource requests and limits, handling graceful shutdowns, logging and monitoring, and appropriate container images.

**Pods as logical host**
A Pod can run one or more closely-related containers which share the same network and storage context. This shared context is much like what you would find on a physical or virtual machine, hence the term "logical host." 
The key points to understand about a Pod as a logical host are:
- **Tightly coupled containers:** When multiple containers within a Pod are considered tightly coupled, it means they have a strong interdependency and need to communicate with each other over localhost. This allows them to exchange data and information efficiently without the need for complex networking configurations.

- **Shared network namespace**: Containers within the same Pod share the same network namespace. This implies that they have the same IP address and port space, making it easier for them to communicate using standard inter-process communication mechanisms.

- **Shared storage context:** Pods also share the same storage context, which means they can access the same volumes or storage resources. This facilitates data sharing among the containers within the Pod, further enhancing their collaboration.

- **Co-location and co-scheduling:** Kubernetes ensures that all containers within a Pod are scheduled and co-located on the same node. This co-scheduling ensures that the containers can efficiently communicate with each other within the same network and storage context.

- **Ephemeral nature:** Like individual containers, Pods are considered to be ephemeral and can be easily created, terminated, or replaced based on scaling requirements or resource constraints. However, all containers within the Pod are treated as a single unit in terms of scheduling and lifecycle management.

**Pods in action**
Let's say you're a software developer in charge of a web application that includes a main web server and a helper component for log processing. The web server interacts with the log processor to handle, analyze, and store log data in real-time. These two components need to be tightly integrated and should communicate with each other efficiently.

In this scenario, you would use a Kubernetes Pod to encapsulate both the web server and the log processor containers. Since both containers exist within the same Pod, they share the same network namespace (they can communicate via localhost) and they can share the same storage volumes. This allows the web server to generate logs and the log processor to access and process these logs efficiently.

The Pod ensures that both the web server and log processor are scheduled on the same node (co-location) and managed as a single entity. If the Pod needs to be rescheduled or if it fails, both containers would be dealt with together, maintaining their coupled relationship. The Pod abstracts away the details of the host machine and the underlying infrastructure, allowing you to focus on managing your application.

This setup, where multiple related containers are grouped in a Pod, is known as a multi-container Pod. You’ll explore single– and multiple-container pods in more detail below; for now, just know that multiple containers are an ideal way to manage and deploy tightly coupled application components. 


**Advantages of Pods**
From the above example, you can see that Pods offer a number of advantages in facilitating co-location of containers, enabling data sharing, and simplifying inter-container communication:
- **Facilitating co-location:** Pods allow multiple containers to be co-located on the same host machine. This is particularly useful for closely related components that need to work together, such as an application and its helper components (like sidecar containers that handle logging or monitoring). By running these components in the same Pod, they can be scheduled onto the same machine and managed as a single entity.

- **Enabling data sharing:** Containers within a Pod share the same network namespace, which means they share an IP address and port space. They can communicate with each other using localhost and they can also share data through shared volumes. Shared volumes in a Pod enable data to be easily exchanged between containers, and also allow data to persist beyond the life of a single container, which can be useful for applications that require persistent data storage.

- **Simplifying inter-container communication:** The shared network namespace also simplifies inter-container communication. Because all containers in a Pod share a network stack, they can communicate with each other on localhost, without the need for inter-process communication (IPC) or shared file systems. This simplifies the development of distributed systems, where components often need to communicate with each other.

**Single container vs. multiple containers**
The difference between single-container and multi-container Pods lies in the number of containers they host.

As the name suggests, **single-container Pods** contain only one container. This container typically represents the primary application or service that the Pod is meant to run. Single-container Pods are straightforward and are commonly used when you have a simple application that requires no additional sidecar containers or closely related helper components. They are also suitable for running standalone applications that do not need to communicate with other containers within the same Pod. 

**Multi-container Pods**, on the other hand, contain multiple containers that are co-located and share the same resources and network namespace. These containers are meant to work together and complement each other's functionalities. Multi-container Pods are appropriate in various scenarios:  	
- **Sidecar pattern:** The sidecar pattern is a common use case for multi-container Pods. In this pattern, the main container represents the primary application, while additional sidecar containers provide supporting features like logging, monitoring, or authentication. The sidecar containers enhance and extend the capabilities of the main application without modifying its code.

- **Proxy pattern:** Multi-container Pods can use a proxy container that acts as an intermediary between the main application container and the external world. The proxy container handles tasks like load balancing, caching, or SSL termination, offloading these responsibilities from the main application container.

**Adapter pattern:** Multi-container Pods can employ an adapter container that performs data format conversions or protocol translations. This allows the main container to focus solely on its core functionality without worrying about the intricacies of data exchange formats.

**Shared data and dependencies:** Containers within a multi-container Pod can share volumes and communicate over localhost, making them suitable for applications that require data sharing or have interdependent components.

Use a single-container Pod when you have a simple application that does not require additional containers, or when you want to isolate different applications or services for easier management and scaling. 

Use multi-container Pods when you have closely related components that need to work together, such as those following the sidecar pattern. This is useful for tasks like logging, monitoring, or enhancing the main application's capabilities without modifying its code. Multi-container Pods are also appropriate for scenarios where multiple containers need to share data or dependencies efficiently.


**Key terms**
Here are some key terms to be familiar with as you’re working with Kubernetes.
- **Pod lifecycle:** Pods have specific lifecycle phases, starting from "Pending" when they are being scheduled, to "Running" when all containers are up and running, "Succeeded" when all containers successfully terminate, and "Failed" if any container within the Pod fails to run. Pods can also be in a "ContainerCreating" state if one or more containers are being created.

**Pod templates:** Pod templates define the specification for creating new Pods. They are used in higher-level controllers like ReplicaSets, Deployments, and StatefulSets to ensure the desired state of the Pods.

**Pod affinity and anti-affinity:** Pod affinity and anti-affinity rules define the scheduling preferences and restrictions for Pods. They allow you to influence the co-location or separation of Pods based on labels and other attributes.

**Pod autoscaling:** Kubernetes provides Horizontal Pod Autoscaler (HPA) functionality that automatically scales the number of replicas (Pods) based on resource usage or custom metrics.

**Pod security policies:** Pod security policies are used to control the security-related aspects of Pods, such as their access to certain host resources, usage of privileged containers, and more.

**Init containers: **Init containers are additional containers that run and complete before the main application containers start. They are useful for performing initialization tasks, such as database schema setup or preloading data.

**Pod eviction and disruption:** Pods can be evicted from nodes due to resource constraints or node failures. Understanding Pod eviction behavior is important for managing application reliability.

**Pod health probes:** Kubernetes supports different types of health probes (liveness, readiness, and startup probes) to check the health of containers within a Pod. These probes help Kubernetes decide whether a Pod is considered healthy and ready to receive traffic.

**Taints and tolerations:** Taints are applied to nodes to repel Pods, while tolerations are set on Pods to allow them to be scheduled on tainted nodes.

**Pod DNS:** Pods are assigned a unique hostname and IP address. They can communicate with each other using their hostname or service names. Kubernetes provides internal DNS resolution for easy communication between Pods.

**Pod annotations and labels:** Annotations and labels can be attached to Pods to provide metadata or facilitate Pod selection for various purposes like monitoring, logging, or routing.

***

Services
---

***
Services offer an abstraction layer over Pods. For starters, they provide a stable virtual IP and a DNS name for each set of related Pods (like your caching layer or database), and these remain constant regardless of the changes in the underlying Pods. So, your web server only needs to know this Service IP or DNS name, saving it from the ordeal of tracking and updating numerous changing Pod IPs.

Furthermore, Services automatically set up load balancing. When your web server sends a request to the caching layer's Service, Kubernetes ensures the request is distributed evenly among all available caching Pods. This automatic load balancing allows for efficient use of resources and improved performance.

In essence, a Service acts like a stable intermediary within the cluster. Instead of applications (like a front-end interface) directly addressing specific Pods, they communicate with the Service. The Service then ensures the request reaches the right backend Pods. This layer of abstraction streamlines intra-cluster communication, making the system more resilient and easier to manage—even as the underlying Pod configurations change dynamically.

**Types of Services**
Let's imagine that, with the basic challenges addressed, you've expanded your Python web application and it now includes a user interface, an API layer, a database, and an external third-party service. Different components of your application have different networking needs, and Kubernetes services, with their various types, can cater to these needs effectively.

First, you have the ClusterIP service. This is the default type and serves as the go-to choice when you need to enable communication between components within the cluster. For example, your API layer and your database might need to communicate frequently, but these exchanges are internal to your application. A ClusterIP service would give you a stable, cluster-internal IP address to facilitate this communication.

Next, you may want to expose your API layer to external clients. You could use a NodePort service for this purpose. It makes your API layer available on a specific port across all nodes in your cluster. With this setup, anyone with access to your node's IP address can communicate with your API layer by contacting the specified NodePort.

However, a NodePort might not be enough if your application is hosted in a cloud environment and you need to handle large volumes of incoming traffic. A LoadBalancer service might be a better choice in this scenario. It exposes your service using your cloud provider's load balancer, distributing incoming traffic across your nodes, which is ideal for components like your user interface that might experience heavy traffic.

Finally, you might be integrating an external third-party service into your application. Rather than expose this service directly within the cluster, you can use an ExternalName service. This gives you an alias for the external service that you can reference using a Kubernetes DNS name.

**In summary,** Kubernetes provides different types of services tailored to various networking requirements:
- **ClusterIP:** Facilitates internal communication within the cluster

- **NodePort:** Enables external access to services at a static port across nodes

- **LoadBalancer:** Provides external access with load balancing, often used with cloud provider load balancers

- **ExternalName:** Serves as an alias for an external service, represented with a Kubernetes DNS name

**Other features**
So far we’ve just scratched the surface of services. There are several features that extend the capabilities of services and can be employed to address specific use cases within your application's networking requirements. 

- **Service discovery with DNS:** As your application grows, new services are added and existing ones might move around as they are scheduled onto different nodes. Kubernetes has a built-in DNS service to automatically assign domain names to services. For instance, your web server could reach the database simply by using its service name (e.g., database-service.default.svc.cluster.local), rather than hard-coding IP addresses.

- **Headless services:** Let's say you want to implement a distributed database that requires direct peer-to-peer communication. You can use a headless service for this. Unlike a standard service, a headless service doesn't provide load-balancing or a stable IP, but instead returns the IP addresses of its associated pods, enabling direct pod-to-pod communication.

- **Service topology:** Suppose your application is deployed in a multi-region environment, and you want to minimize latency by ensuring that requests are served by the nearest pods. Service topology comes to the rescue, allowing you to preferentially route traffic based on the network topology, such as the node, zone, or region.

- **External Traffic Policy:** If you want to preserve the client source IP for requests coming into your web server, you can set the External Traffic Policy to "Local". This routes the traffic directly to the Pods running on the node, bypassing the usual load balancing and ensuring the original client IP is preserved.

- **Session affinity (sticky sessions):** Suppose users log into your application, and their session data is stored locally on the server pod handling the request. To maintain this session data, you could enable session affinity on your service, so that all requests from a specific user are directed to the same pod.

- **Service slicing:** Imagine you're rolling out a new feature and want to test it with a subset of your users. Service Slicing enables you to direct traffic to different sets of pods based on custom labels, providing granular control over traffic routing for A/B testing or canary releases.

- **Connecting external databases:** Perhaps your application relies on an external database hosted outside the Kubernetes cluster. You can create a Service with the type ExternalName to reference this database. This allows your application to access the database using a DNS name without needing to know its IP address, providing a level of indirection and increasing the flexibility of your application configuration.

***
Practice Quiz: Kubernetes
---
***
1. What are some of the advantages of Kubernetes? Select all that apply.
*Ans =>
Kubernetes has become a de facto industry standard;
Kubernetes adds self-healing features (like fault tolerance and load balancing) across multiple servers.*

2. What is the easiest tool for local developers using Windows or macOS to learn Kubernetes? 
Ans => Docker Desktop

3. In Kubernetes, what is a Pod? Select all that apply.
*Ans => 
A Pod is the smallest deployable unit in Kubernetes, consisting of one or more containers;
A Pod is a Kubernetes resource that defines the desired state of one or more containers and is used by higher-level controllers.*

4. What is the purpose of a Kubernetes Service? 
*Ans => To provide stable network endpoints for Pods* 

5. What is the primary purpose of a Kubernetes Deployment?
*Ans => To provide declarative updates and automate the management of replica sets of Pods*